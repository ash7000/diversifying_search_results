---
title: "Applying Community Detection"
author: "Ashwini Ashok Munnolli"
date: "25/08/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r message=FALSE, warning=FALSE}
library(igraph)
library(RColorBrewer)
library(scales)


#my_color_pal <- c("#dc661b", "#c9da0b", "#2eb83c", "#157184", "#7a8fe1", "#5525a2","#fd74d8","#b948d5","#c79750",'#939290', "#6e4a24")


#add.alpha <- function(cols, alpha) rgb(t(col2rgb(cols)/255), alpha = alpha)
#colours<-add.alpha(my_color_pal, 0.9)
colours <- brewer.pal(n=12,name = "Paired")

show_col(colours)

```




```{r}

#code block for single matrix

#check read.table doc skip param
data <- read.table("similarity_matrix_for_graph.csv",sep = ',',header = TRUE, check.names = FALSE, skip = 51 * 3 , nrows = 50)

#DF to matrix
data_matrix <- data.matrix(data)
#colnames of the matrix
colnames(data_matrix)

#making all lower triangle values zero in the matrix including diag
data_matrix[lower.tri(data_matrix,diag = TRUE)] <- 0

#number of zeros in the matrix
#length(which(data_matrix == 0))

#avg of non-zero values in the matrix
sum(data_matrix[data_matrix != 0])/length(data_matrix[data_matrix != 0])

summary(data_matrix[data_matrix != 0])

fivenum(data_matrix[data_matrix != 0])


#3rd quadrant
threshold <- fivenum(data_matrix[data_matrix != 0])[4]

#number of values less than threshold (including zeros and lower tria)
length(which(data_matrix >= threshold))

#filter out values below threshold
data_matrix[data_matrix < threshold] <- 0
#data_matrix[data_matrix >= threshold] <- 1
#length(which(data_matrix == 0))
#weighted check
g <- graph_from_adjacency_matrix(data_matrix, weighted=TRUE, mode="upper", diag = FALSE, add.colnames = "medline_ui")
#V(g)
#V(g)$medline_ui
#E(g)
#E(g)$weight

summary(g)

#summary(E(g)$weight)

#fivenum(E(g)$weight)[2]


#plot(g)
```

```{r}


vertex_size <- 10 
cex_size <-0.6
par(mar=c(0,0,1,0)+.1)
plot( g,
layout=layout_nicely,
vertex.label.cex=cex_size,
vertex.label.color="black",
#vertex.label.dist = 0.7,
vertex.label.font = 2,
vertex.size = vertex_size,
#edge.color = "gray90",
edge.width=0.5,
edge.curved=0.2,
main = paste("Sample graph")
)
```



```{r}


#By default the ‘weight’ edge attribute is used as weights.Larger edge weights correspond to stronger connections.
fg.community<- cluster_fast_greedy(g) 
table(fg.community$membership)

#modularity of actual split
modularity(fg.community)
#modularity of actual split
modularity(g,membership(fg.community),E(g)$weight)

#plot(as.dendrogram(as.hclust((fg.community))))

#is_hierarchical(fg.community)
fg.10_comm <- cut_at(fg.community,no = 10)
table(fg.10_comm)

#modularity of cut at 10
modularity(g,fg.10_comm,E(g)$weight)
#length(fg.10_comm)

#weights: Optional positive weight vector. If the graph has a weight edge attribute, then this is used by default.
#louvain.community<- cluster_louvain(g)
#length(louvain.community)



```

```{r}

 

par(mar=c(0,0,1,0)+.1)
plot( g,
layout=layout_nicely,
vertex.color=colours[fg.10_comm],
vertex.label.cex=cex_size,
vertex.label.color="black",
#vertex.label.dist = 0.7,
vertex.label.font = 2,
vertex.size = vertex_size,
#edge.color = "gray90",
edge.width=0.5,
edge.curved=0.2,
main = paste("Cut at 10")
)
```

```{r}


par(mar=c(0,0,1,0)+.1)
plot( g,
layout=layout_nicely,
vertex.color=colours[fg.community$membership],
vertex.label.cex=cex_size,
vertex.label.color="black",
#vertex.label.dist = 0.7,
vertex.label.font = 2,
vertex.size = vertex_size,
#edge.color = "gray90",
edge.width=0.5,
edge.curved=0.2,
main = paste("Actual communities")
)
```

```{r}
#code for all 63 queries
# to find clusters and new top 10 docs

actual_clusters =  c()
final_clusters = c()

orig_top_50 <- list()
new_top_10 <- list()


for (i in 1:63) {

data <- read.table("similarity_matrix_for_graph.csv",sep = ',',header = TRUE, check.names = FALSE, skip = 51 * (i-1) , nrows = 50)

#DF to matrix
data_matrix <- data.matrix(data)

#colnames of the matrix
#colnames(data_matrix)

#making all lower triangle values zero in the matrix including diag
data_matrix[lower.tri(data_matrix,diag = TRUE)] <- 0


#3rd quadrant
threshold <- fivenum(data_matrix[data_matrix != 0])[4]

#filter out values below threshold
data_matrix[data_matrix < threshold] <- 0

#weighted check
g <- graph_from_adjacency_matrix(data_matrix, weighted=TRUE, mode="upper", diag = FALSE, add.colnames = "medline_ui")


#summary(g)


fg.community <- cluster_fast_greedy(g)
actual_clusters[i] <- length(fg.community)
#sizes(fg.community)

if(length(fg.community) >= 10)
{
  final_clusters[i] <- length(fg.community)
  print(sizes(fg.community))
  print(fg.community$membership)
  mem_vector <- fg.community$membership
  n_comm <- length(fg.community)
}
else
{
  fg.10_comm <- cut_at(fg.community,no = 10)
  final_clusters[i] <- 10
  print(table(fg.10_comm))
  print(fg.10_comm)
  mem_vector <- fg.10_comm
  n_comm <- 10
}

#initialisations
index <- c(1)
seen_clusters <- c(mem_vector[1])

for (n in 2:50)
{
  if (!(mem_vector[n] %in% seen_clusters))
  {
    seen_clusters <- append(seen_clusters,mem_vector[n])
    index <- append(index,n)
  }
  
}


#taking only top 10
new_top_10 <- append(new_top_10,list(index[1:10]))
orig_top_50 <- append(orig_top_50,list(V(g)$medline_ui))

}


length(which(actual_clusters <10))
actual_clusters
mean(actual_clusters)

final_clusters

```
```{r}
#read rel_results of top k from file
rel_results <- read.table("relevant_doc_list_top_50_for_63q.csv",sep = ',',header = FALSE, nrows = 63)

#rel_results
new_rel_list_10 = list()
new_rel_df_10 <- data.frame()
for (j in 1:63)
{
  rel_vec <- c()
  for (k in 1:10)
  {
    if (new_top_10[[j]][k] %in% (which(rel_results[j,] == 1)))
    {
      rel_vec <- append(rel_vec,1)
    }
    else
    {
      rel_vec <- append(rel_vec,0)
    }
  }
  new_rel_list_10 <- append(new_rel_list_10,list(rel_vec))
  new_rel_df_10 <- rbind(new_rel_df_10,rel_vec)
}


# write new rel_list to file for jupyter to read
write.table(new_rel_df_10,'new_relevant_docs_top_10.csv',sep =',',row.names = FALSE,col.names = FALSE)
```


```{r}

#Fetching similarity scores before and after 

#new_top_10
#orig_top_50
sum_old = 0
sum_new = 0
for (i in 1:63) {

sim_matrix <- read.table("similarity_matrix_for_graph.csv",sep = ',',header = TRUE, check.names = FALSE, skip = 51 * (i-1) , nrows = 50)

#DF to matrix
data_matrix_k50 <- data.matrix(sim_matrix)
#print(data_matrix_k50)


#old sim matrix
#first subtract 10 because we dont want similarity if i,i
print(i)
print((sum(data_matrix_k50[1:10,1:10]) - 10) / 90)


#new sim matrix
print((sum(data_matrix_k50[new_top_10[[i]],new_top_10[[i]]]) - 10)/ 90)
if ((sum(data_matrix_k50[1:10,1:10]) - 10)/ 90 < (sum(data_matrix_k50[new_top_10[[i]],new_top_10[[i]]]) - 10)/ 90)
{
  print("Similarity score did not decrease")
}


sum_old <- sum_old + (sum(data_matrix_k50[1:10,1:10]) - 10)/ 90

sum_new <- sum_new + (sum(data_matrix_k50[new_top_10[[i]],new_top_10[[i]]]) - 10)/ 90

}


#mean of (avg (sim score for every i,j))
print("Before and after")
sum_old/63

sum_new/63


```

